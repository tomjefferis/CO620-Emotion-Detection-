{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook includes all the code in order to extract features and classify them"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot, gridspec\n",
    "import neurokit2 as nk2\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import scipy as sci\n",
    "import pickle\n",
    "import os\n",
    "import heartpy as hp\n",
    "import GazeParser\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Starting with importing the raw ecg & gsr data into dataframes, it also excludes the trials with technical difficulty"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = []\n",
    "# Getting all the files in the data directory\n",
    "files = os.listdir(\"./raw_data/ecg-gsr-labels/\")\n",
    "removed = ['101_PreProcessed_GSR_ECG.dat', '102_PreProcessed_GSR_ECG.dat', '103_PreProcessed_GSR_ECG.dat', '115_PreProcessed_GSR_ECG.dat', '118_PreProcessed_GSR_ECG.dat', '121_PreProcessed_GSR_ECG.dat', '119_PreProcessed_GSR_ECG.dat', '130_PreProcessed_GSR_ECG.dat']\n",
    "\n",
    "for item in removed:\n",
    "    files.remove(item)\n",
    "\n",
    "for i in range(len(files)):\n",
    "    infile = open(\"./raw_data/ecg-gsr-labels/\" + files[i],'rb')\n",
    "    data.append(pickle.load(infile))\n",
    "    infile.close()\n",
    "\n",
    "# Extracting all data (labels, ecg and gsr data) into seperate arrays.\n",
    "\n",
    "completeLabels = []\n",
    "completeEcg = []\n",
    "completeGsr = []\n",
    "\n",
    "# Iterate over all files\n",
    "for i in range(len(data)):\n",
    "    del data[i]['Data'][0]\n",
    "    del data[i]['Labels'][0]\n",
    "    features = data[i]['Data']\n",
    "    labels = data[i]['Labels']\n",
    "    #Iterate over all examples in file\n",
    "    for x in range(len(labels)):\n",
    "        completeLabels.append(labels[x])\n",
    "        completeEcg.append(features[x][:][:,1])\n",
    "        completeGsr.append(features[x][:][:,0])\n",
    "\n",
    "pd.DataFrame.from_dict(completeLabels).to_csv('processed_data/labels.csv')\n",
    "labelslen = len(completeLabels)\n",
    "ecglen = len(completeEcg)\n",
    "gsrlen = len(completeGsr)\n",
    "print(f\"Completed:{labelslen} lables, {ecglen} ECG inputs, {gsrlen} GSR inputs\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have the ecg, gsr and labels we can extract the eye tracking data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "# Getting all the files in the data directory\n",
    "files = os.listdir(\"./raw_data/eye-track/\")\n",
    "\n",
    "completeEyeLabels = []\n",
    "completeData = []\n",
    "\n",
    "for i in range(len(files)):\n",
    "    infile = open(\"./raw_data/eye-track/\" + files[i],'rb')\n",
    "    data.append(pickle.load(infile))\n",
    "    infile.close()\n",
    "for i in range(len(data)):\n",
    "    eyefeatures = data[i]['Data']\n",
    "    eyelabels = data[i]['Labels']\n",
    "    #Iterate over all examples in file\n",
    "    for x in range(len(eyelabels)):\n",
    "        completeEyeLabels.append(eyelabels[x])\n",
    "        completeData.append(eyefeatures[x])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now were extracting the ECG and GSR Features manually and using Neurokit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def nkExtractECG(signal,samplerate):\n",
    "    cleaned = nk2.ecg_clean(signal,sampling_rate=samplerate)\n",
    "    processed, info = nk2.ecg_process(cleaned,sampling_rate=samplerate)\n",
    "    compECG = nk2.ecg_intervalrelated(processed)\n",
    "    return compECG\n",
    "\n",
    "def extractECGfeatures(samplerate):\n",
    "    print(\"started ECG extraction\")\n",
    "    # Preprocess the data (filter, find peaks, etc.)\n",
    "    completeFrame = pd.DataFrame([])\n",
    "\n",
    "    for index, ecg in enumerate(completeEcg): #loops through all 312 data items from the extracted items\n",
    "        tems = nkExtractECG(ecg,samplerate)\n",
    "        #temp = pd.DataFrame([tems])\n",
    "        completeFrame = pd.concat([completeFrame,tems])\n",
    "    #completeFrame.drop(completeFrame.columns[0], axis=1,inplace=True) #removes first empty column\n",
    "    completeFrame.to_csv('processed_data/ecgNK.csv')\n",
    "    print(\"Completed ECG extraction\")\n",
    "\n",
    "\n",
    "def extractGSR(signal,samplerate):\n",
    "    signal = hp.filter_signal(signal, 0.1, 1000)\n",
    "    processed_gsr,infos = nk2.eda_process(signal,sampling_rate=samplerate) #processes the GSR, currently only doing one item to make sure it works properly\n",
    "    #plot = nk2.eda_plot(processed_gsr[:30000], sampling_rate=1000) #plots the signal on a graph\n",
    "\n",
    "    gsr_dict = nk2.eda_findpeaks(processed_gsr)#finding peaks, time of the peaks and magnitude of peaks\n",
    "\n",
    "    numpeakssci = len(sci.signal.find_peaks(signal)[0])/len(signal)\n",
    "    numvalleys = len(sci.signal.find_peaks(signal*(-1))[0])/len(signal)\n",
    "\n",
    "    peaktime = gsr_dict['SCR_Peaks'] #time of peaks\n",
    "    numpeaks = len(peaktime)/len(signal) #number of peaks\n",
    "    timebetween = []\n",
    "    lastPeak = 0\n",
    "    for peaks in peaktime: # going through all the peaks in the exrtacted data\n",
    "        if lastPeak != 0:\n",
    "            timebetween.append(peaks-lastPeak) #finding the time between the peaks\n",
    "        else:\n",
    "            lastPeak = peaks\n",
    "\n",
    "    if len(timebetween) >= 1:\n",
    "        meantbpeaks = statistics.mean(timebetween) #mean time between peaks\n",
    "        mediantbpeaks = statistics.median(timebetween) # median time between peaks\n",
    "        meanheightpeaks = statistics.mean(gsr_dict['SCR_Height']) #mean magnitide of peaks\n",
    "        medianheightpeaks = statistics.median(gsr_dict['SCR_Height']) #median magnitide of peaks\n",
    "        varheightpeaks = statistics.variance(gsr_dict['SCR_Height']) #variance magnitide of peaks\n",
    "    elif len(gsr_dict['SCR_Height']) >= 1:\n",
    "        meantbpeaks = 0 #mean time between peaks\n",
    "        mediantbpeaks = 0 # median time between peaks\n",
    "        meanheightpeaks = statistics.mean(gsr_dict['SCR_Height']) #mean magnitide of peaks\n",
    "        medianheightpeaks = statistics.median(gsr_dict['SCR_Height']) #median magnitide of peaks\n",
    "        if len(gsr_dict['SCR_Height']) >= 2:\n",
    "            varheightpeaks = statistics.variance(gsr_dict['SCR_Height']) #variance magnitide of peaks\n",
    "        else:\n",
    "            varheightpeaks = 0\n",
    "    else:\n",
    "        meantbpeaks = 0 #mean time between peaks\n",
    "        mediantbpeaks = 0 # median time between peaks\n",
    "        meanheightpeaks = 0 #mean magnitide of peaks\n",
    "        medianheightpeaks = 0 #median magnitide of peaks\n",
    "        varheightpeaks = 0 #variance magnitide of peaks\n",
    "\n",
    "    maxGSR = max(signal)\n",
    "    minGSR = min(signal)\n",
    "    meanGSR = statistics.mean(signal)\n",
    "    peakratio = 0\n",
    "    if numpeaks>0: peakratio = numpeaks/len(signal)\n",
    "    valleyratio =0\n",
    "    if numvalleys>0: valleyratio = numvalleys/len(signal)\n",
    "\n",
    "    d = {'max GSR': maxGSR,\n",
    "         'min GSR': minGSR,\n",
    "         'mean GSR': meanGSR,\n",
    "         'number of peaks':numpeaks,\n",
    "         'number of peaks SCIPY': numpeakssci,\n",
    "         'number of valleys': numvalleys,\n",
    "         'mean time between peaks':meantbpeaks,\n",
    "         'median time between peaks':mediantbpeaks,\n",
    "         'mean height of peaks': meanheightpeaks,\n",
    "         'median height of peaks': medianheightpeaks,\n",
    "         'variance height of peaks': varheightpeaks,\n",
    "         'ratio of peaks': peakratio,\n",
    "         'ratio of valleys': valleyratio}\n",
    "    #adding to dataframe to be stored as csv later\n",
    "    return pd.DataFrame([d])\n",
    "\n",
    "def extractGSRfeatures(samplerate):\n",
    "    print(\"started GSR extraction\")\n",
    "     # Preprocess the data (filter, find peaks, etc.)\n",
    "    completeFrame = pd.DataFrame([])\n",
    "\n",
    "    for index, gsr in enumerate(completeGsr): #loops through all 398 data items from the extracted items\n",
    "\n",
    "        temp = extractGSR(gsr,samplerate)\n",
    "        completeFrame = pd.concat([completeFrame,temp])\n",
    "    #completeFrame.drop(completeFrame.columns[0], axis=1,inplace=True) #removes first empty column\n",
    "    completeFrame.to_csv('processed_data/gsrNK.csv')\n",
    "    print(\"Completed GSR extraction\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "samplerate = 1000\n",
    "#extractGSRfeatures(samplerate)\n",
    "#extractECGfeatures(samplerate) #uncomment to run extraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I am forming the datasets to be used in the machine learning process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "ecg = pd.read_csv('./processed_data/ecgNK.csv')\n",
    "gsr = pd.read_csv('./processed_data/gsrNK.csv')\n",
    "labels = pd.read_csv('./given_data/labels.csv')\n",
    "eyes = pd.read_csv('./given_data/eye.csv')\n",
    "\n",
    "#removing unnecessary columns from data and concatenating into one dataset\n",
    "ecg.drop(['HRV_ULF','HRV_VLF'],axis=1, inplace=True)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "#scaling the ecg and gsr data using minmax scaler\n",
    "ecgLab = ecg.columns.values.tolist()\n",
    "ecgScale = min_max_scaler.fit_transform(ecg)\n",
    "ecg = pd.DataFrame.from_dict(ecgScale)\n",
    "ecg.columns= ecgLab\n",
    "\n",
    "gsrLab = list(gsr.columns.values)\n",
    "gsrScale = min_max_scaler.fit_transform(gsr)\n",
    "gsr = pd.DataFrame.from_dict(gsrScale)\n",
    "gsr.columns = gsrLab\n",
    "\n",
    "ds1 = pd.concat([ecg,gsr,eyes,labels], axis=1, join='inner').sort_index()\n",
    "ds1.to_csv('processed_data/completeData.csv', index=False)\n",
    "\n",
    "ds1 = pd.concat([ecg,gsr,labels], axis=1, join='inner').sort_index()\n",
    "ds1.to_csv('processed_data/ecggsr.csv', index=False)\n",
    "\n",
    "ds3 = pd.concat([ecg,gsr,labels], axis=1, join='inner').sort_index()\n",
    "ds3.loc[ds3[\"labels\"] == 1, \"labels\"] = 0\n",
    "ds3.loc[ds3[\"labels\"] == 2, \"labels\"] = 1\n",
    "ds3.loc[ds3[\"labels\"] == 3, \"labels\"] = 1\n",
    "ds3.to_csv(\"./processed_data/completeDatabinarynoeye.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Starting machine learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import performance_metrics #This python script does the machine learning across 9 algorithms, you pass in the name of the dataset and the location and it outputs results\n",
    "\n",
    "results = {\"databinarynoeye\": \"./processed_data/completeDatabinarynoeye.csv\",\"completeData\": \"././processed_data/completeData.csv\",\"ecggsr\": \"./processed_data/completeDatanoeye.csv\"}\n",
    "performance_metrics.main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}